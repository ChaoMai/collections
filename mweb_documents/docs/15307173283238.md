---
title: 6.824 2018 Lecture 3: GFS
date: 2019-05-21 08:38:33
categories:
    - "distritubed system"
tags:
    - 6.824
    - mapreduce
---

# Readings - The Google File System论文
## 介绍
GFS是由Google设计和实现的，以满足Google对数据处理快速正常的需求。GFS和先前的分布式文件系统有很多相似的目标，例如：性能、可扩展性、可靠性和可用性。然而，GFS的设计是由Google对应用负载和技术环境的关键（当前和预期的）观察驱动的，这反映了与早期文件系统设计假设的显著不同。Google重新审视传统的选择，并探索在设计领域探索了彻底不同的观点。
* 组件故障是常态而非例外。
    因此持续的监控、错误监测、容错和自动恢复是系统不可缺少的。
* 传统标准的文件是巨大的，常常很多GB。处理包含数十亿个对象、很多TB、且快速增长的数据集时，即便文件系统可以支持，也很难管理数十亿个约KB大小的文件。
    因此设计假设和参数，例如IO操作和block大小必须被重新审视。
* 大多数文件修改都是通过追加新数据的方式，而非覆盖已有的数据。对文件的随机写几乎已经不存在了。一旦写入，文件就是只读的，且常常只是顺序读。
  鉴于这样对大文件的访问模式，追加成为了性能优化和原子保证的的关注点，而在client上对数据块的缓存失去了吸引力。
* 应用程序和文件系统api的协同设计，通过提升灵活性，有益于整个系统。

## 设计概览
### 接口
虽然GFS没有实现例如POSIX的标准API，但也提供了一组熟悉的文件系统接口。文件以层次结构的方式用目录组织起来，并用路径名来标识。GFS支持*create，delete，open，close，read*和*write*文件。GFS还支持*snapshot*和*record append*。

### 架构
一个GFS集群由一个*master*和多个*chunkserver*构成，可以被多个*client*访问。

![-w757](media/15307173283238/15584874415545.jpg)

文件被切分为固定大小的*chunks*。每个chunk都由一个不可变，且全局唯一的64位*chunk handle*标识，这个chunk handle在创建chunk时master分配的。chunkservers把chunk作为linux文件存储在本地磁盘上，并通过指定的chunk handle和byte range来读写文件。为了reliability（可靠性），每个chunk会在多个chunkservers上有复制。

master维护这整个文件系统的metadata，包括namespace，访问控制信息，文件到chunk的映射，以及chucks的当前位置。master还可控制着系统级别的活动，例如chuck租约管理，孤儿chunk的垃圾回收，以及chunkservers之间的chunk迁移。master定期与每个chunkserver以HeartBeat消息的形式进行通信，完成指令的发送和状态的收集。

client与master通信来进行metadata相关的操作，数据相关的通信是直接与chunkserver进行的。

client和chunkserver都不需要缓存数据。client缓存数据的收益很小，因为大多数程序都是流式读取大文件或者数据量太大而无法缓存。client不用缓存数据消除了缓存一致性的问题，简化了系统设计。但client会缓存metadata。chunkserver不必缓存文件数据，因为chunk都是以本地文件的形式保存的，linux buffer cache会把常访问的数据放入内存。

### 单一master
单一master极大的简化了设计，并使master能够使用全局信息来进行复杂的chunk布局和复制决策。然而当读写时，必须最小化master的参与，这样master才不会成为瓶颈。client进行读写时，先询问master应该连接哪个chunkserver，并缓存这个信息一段时间，然后直接与这个chunkserver交互来完成后续的操作。

### chunk size
chunk size是关键设计参数之一。GFS使用一个远大于典型文件系统的block size，64MB。每个chunk副本都以普通linux文件的形式存储在chunkserver，并在需要的时候扩展。惰性空间分配避免了由于内部碎片导致的空间浪费。

大型chunk size有这些优势，
* 减少了client与master交互的需求。
    读写同一个chunk只需要向master请求一次chunk的位置信息。
* 由于chunk较大，client也较为可能在一个给定的chunk上进行很多的操作。
    通过在较长时间内保持与chunkserver的TCP连接，可以减少网络开销。
* 减少了存储在master上metadata的大小。
    由此可以将metadata放入内存。

然而大型chunk size，即便有惰性空间分配，也存在弊端，
* chunkserver的热点访问。
    一个小文件仅有为数不多的chunks组成，可能就一个。如果大量的client都访问同一个文件，那么存储这些chunk的chunkserver可能会成为热点。

### Metadata
mater主要存储3类metadata：
* 文件和chunk namespace
* 文件到chunks的映射关系
* 每个chunk副本的位置

所有的metadata都是存储在内存中的。
* 前两种也会进行持久化存储，这是通过把记录修改到操作日志、在master落盘、以及在远程机器上存放副本来实现的。
* 对于最后一种metadata，master并不会做持久化存储。在master启动和有chunkserver加入集群的时候，master会询问每一个chunkserver存放的chunks。

#### In-Memory Data Structures
由于metadata是存放在内存中的，因此mater的操作很快，除此之外还能完成定期在后台较快的完整扫描。这个定期扫描用于实现chunk的gc，chunkserver故障时副本重新复制，以及chunk的迁移。

一个潜在的问题是存储的chunk数目受限于内存的大小。但由于每个chunk的metadata少于64byte，且文件的namespace数据也少于64byte，并且启用了前缀压缩，因此这不是一个严重的问题。如果确实有必要支持更大的文件系统，加内存即可。

#### Chunk Locations
master并不对chunk locations做持久化。master启动时，会向所有chunkserver请求。之后，由于master控制着所有chunks的放置，并通过心跳消息来监控chunkserver，master能够确保自身的信息是最新的。

为什么不做持久化？
1. 消除了master和chunkserver的同步问题。
    chunkserver可能加入、离开、重启、重命名、故障等。
2. chunkserver对自己有和没有哪些chunk有最终的话语权。
    在master维护此信息的一致视图是没有意义的，chunkser可能出现1中的各种问题。
    
#### Operation Log
操作日志对GFS很重要，
* 包含了metadata关键修改的历史记录，并持久化。
* 作为逻辑时间戳，定义了并发操作的顺序。
    文件和chunks，以及它们的版本，全都在创建的时候被逻辑时间唯一且永久的标识。
    
可靠性保证，
* 仅当metadata的修改完成持久化以后，这些修改才对client可见。
* 在多个远程机器上有复制。
    * 仅当把相应的log记录写入本地和远程机器的磁盘后，才响应client。
    * master会批量flush日志，来减少flush和复制对整个集群吞吐量的影响。
* master通过重放操作日志来恢复文件系统的状态。
    * 为了减少启动的时间，需要保证log较小。
    * 当log增长超过特定大小时，master会checkpoint自身的状态，以便可以在恢复时载入最后一个checkpoint并重放在那之后的log。

checkpoint和恢复，
* checkpoint类似于压缩后的B树，可以直接map到内存，并用用户namespace的查找，且不需要额外的解析。
* master以一种。
* 恢复只需要最近的一个*完整*（需要检测是否完整）checkpoint和后续的日志文件。

### 一致性模型

# Lectures

# Lab
* Lecture 3没有关于GFS的实验，不过我找到了[ppca-gfs](https://bitbucket.org/abcdabcd987/ppca-gfs/src/master/)，看介绍是上交ACM班一个课程的作业，就用这个来补上GFS的实验吧。
* lab的代码在[github.com/chaomai/mit-6.824](https://github.com/chaomai/mit-6.824/tree/master/src/gfs)
