# 统计学
统计学是搜集、分析、展示和解释数据的科学。

# 数据的收集和描述
## 基本概念
以问卷调查为例，每个人称为调查**对象**，每个人的观点称为调查的**个体**，所有人观点的集合称为**总体**。总体是包含了所有个体的集合。调查时问到的那部分人的观点称为总体的**样本**。

## 收集数据的方法
收集数据的方法有，
1. 概率抽样
    假定每个个体出现在样本中的概率是已知的，便于进行合理的统计推断。
    * 简单随机抽样，是最理想的抽样方法。但是实际操作，可行性视情况而定。 
    * 系统抽样，先编号，然后随机选取起始样本，接着往后等距离选取后续样本，间隔视样本量大小而定。
2. 非概率抽样
    不便于进行合理的统计推断，具体依赖抽样方法的设计和实施。
    
## 数据的描述
数据可以使用图标和统计量描述。统计量是从样本计算得到的。由于样本的获取存在随机性，因此统计量会随样本的不同而不同。

统计量分为，
1. 位置统计量
    这类统计量描述数据分布的“中心”。
    * 算数平局值
    * 中位数
    * k百分为数
    * 众数
2. 尺度统计量
    这类统计量描述数据的离散程度。
    * 极差
    * 样本标准差$s$
        由于不同样本会有不同的均值，不同样本均值的标准差成为**标准误差**，定义为$\frac{s}{\sqrt{n}}$。
    * 样本方差$s^2$
3. 数据的标准得分
    用于统一不同评分体系下，样本得分不同的方法。仅适用于两个相似的样本。
    * 标准得分，$z_i = \frac{x_i - \bar{x}}{s}$

# 概率和分布
## 概率
事件是一个明确的**陈述**，这个陈述界定了**所有试验结果**中的**一个确定的部分**。

例如掷一次骰子，
* 陈述：得到偶数点
* 所有试验结果：$(1, 2, ..., 6)$
* 确定的部分：$(2, 4, 6)$

有时把单一的试验结果成为基本事件。

概率是事件发生可能性大小的数量指标。定义有两种：

* 古典概率：基于等可能事件定义。部分问题的试验结果有无穷多种，此时会将**等可能**的定义通过等长度、等面积、等体积...进行引申。
    一个试验包含N个等可能的结果，事件 E 包含其中 M 个，则事件 E 的概率记为 $\Pr (E)$，定义 $\Pr (E) = M /N$。
* 基于统计定义的概率：通过试验的频数来估计概率。

概率的公理化定义：基本事件集合 $\Omega = \{ \omega \mid \omega 是基本事件 \}$；定义事件 $\mathscr{E}$，一个事件由多个基本事件构成，即 $\mathscr{E} \subset \Omega$；定义事件集为 $\mathscr{F} = \{ \mathscr{e} \mid \mathscr{e} \subset  \Omega \} $；在 $\mathscr{F}$ 上定义函数 $P: \mathscr{F} \rightarrow \mathscr{R}$，其中 $\mathscr{R} = [0, 1]$，则 $\Pr (A)$ 为某一事件 $A \in \mathscr{F}$ 的概率，且 $\Pr (\Omega) = 1$，$\Pr (\emptyset) = 0$。

## 古典概率计算
基于定义即可，关键是找出试验和事件对应的基本事件数目。

## 事件的运算
### 事件的蕴含、包含和相等
同一实验下的事件 A 和 B，若 A 发生时 B 必发生，则称 A 蕴含 B，或 B 包含 A，$A \subset B$。若 $A \subset B$，且 $B \subset A$，则 $B = A$。

### 事件的和、积与差
**互斥事件**：同一实验下不能同时发生。
**对立事件**：对于事件 A，对立事件 $\bar{A}$ 表示 A 不发生。

**事件的和**：对于事件 A、B，定义事件 C 为 A、B *至少发生一个*，记为 $C = A + B$。
**事件的积**：对于事件 A、B，定义事件 C 为 A、B *同时发生*，记为 $C = AB$。
**事件的差**：对于事件 A、B，定义事件 C 为 A*发生*、B *不发生*，记为 $C = A - B = A \bar{B}$。
乘法分配律和结合律适用。

对于若干个互斥事件，$A_1, A_2, \cdots$，$P(A_1 + A_2 + \cdots) = \Pr (A_1) + \Pr (A_2) + \cdots$。对于两个互斥事件，$A_, \bar{A}, \Pr (A) = 1 - \Pr (\bar{A})$。

### 条件概率
**条件概率**：设两事件 A 和 B，且$\Pr (B) \neq 0$，则 $\Pr (A \mid B) = \frac{\Pr (AB)}{\Pr (B)} $。若试验有$N$个结果，事件B发生有$M_2$个结果，事件AB有$M_{12}$个结果，那么$Pr(AB) = \frac{M_{12}}{M_2} = \frac{\frac{M_{12}}{N}}{\frac{M_2}{N}} = \frac{\Pr (AB)}{\Pr (B)}$

若两事件 A 和 B 满足 $\Pr (AB) = \Pr (A) \Pr (B) $，则这两个事件相互独立。考虑使用 $\Pr (A) = \Pr (A \mid B) $ 来定义，但此式子受限于 $\Pr (B) \neq 0$。

如果多个事件相互独立，则其中任意两个事件都两两独立，反之不成立。

### 全概率公式
设 $B_1, B_2, \cdots$ 为有限个或无限个事件，彼此两两互斥且每次实验中至少发生一个，即 $B_1 + B_2 + \cdots = \Omega$（必然事件）。这样一组事件也叫做“完备事件群”。

若有事件 A，则有 $A = A \Omega = A B_1 + A B_2 + \cdots$，又因为 $B_1, B_2, \cdots$ 彼此两两互斥，故 $A B_1, A B_2, \cdots$ 也彼此两两互斥。根据加法定理，以及条件概率公式，

$$
\begin{align*}
\Pr (A) & = \Pr (A B_1) + \Pr (A B_2) + \cdots \\
& = \underbrace{\Pr (B_1)}_{(1)} \underbrace{\Pr (A \mid B_1)}_{(2)} + \Pr (B_2) \Pr (A \mid B_2)+ \cdots
\end{align*}
$$

**全概率公式理解**：事件 $B_i$ 是事件 A 发生的不同途径（在 $B_i$ 条件下 A 才会发生，公式（2）处），采取哪个途径是随机的，每个途径的权重（概率，公式（1）处）不同，因此 $\Pr (A)$ 是 $\Pr (A \mid B_i)$ 的加权平均。

## 贝叶斯公式
在全概率的假定下，即 $B_1, B_2, \cdots$ 为有限个或无限个事件，彼此两两互斥且每次实验中至少发生一个，$B_1 + B_2 + \cdots = \Omega$（必然事件）。根据，

$$
\Pr (AB) = \Pr (B) \Pr (A \mid B) \\
\Pr (AB) = \Pr (A) \Pr (B \mid A)
$$

有，

$$
\Pr (B \mid A) = \frac{\Pr (B) \Pr (A \mid B)}{\Pr (A)}
$$

带入 $\Pr (A)$ 全概率公式，对于任意的 $B_i$ 有，

$$
\Pr (B_i \mid A) = \frac{\Pr (B_i) \Pr (A \mid B_i)}{\Pr (A)} = \frac{\Pr (B_i) \Pr (A \mid B_i)}{\sum_{j}{\Pr (B_j) \Pr (A \mid B_j)}}
$$

**贝叶斯公式理解和示例**：结合全概率公式，若事件 A 是结果，事件 $B_i$ 是起因，那么全概率公式是*已知各种原因 $B_i$*，推结果。而贝叶斯公式是，已知结果发生了，找出哪个起因最有可能导致结果发生。

> 一所学校里面有 60% 的男生，40% 的女生。男生总是穿长裤，女生则一半穿长裤一半穿裙子。有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近似，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是女生的概率是多大吗？

> $$
> \begin{align*}
> & \Pr (Boy) = 0.6 \\
> & \Pr (Girl) = 0.4 \\
> & \Pr (Pants \mid Boy) = 1 \\
> & \Pr (Pants \mid Girl) = 0.5
> \end{align*}
> $$

> 根据贝叶斯公式可得，

> $$
> \begin{align*}
> \Pr (Girl \mid Pants) & = \frac{\Pr (Girl) \Pr (Pants \mid Girl)}{\Pr (Girl) \Pr (Pants \mid Girl) + \Pr (Boy) \Pr (Pants \mid Boy)} \\
> & = \frac{\Pr (Girl \cdot Pants)}{\Pr (Pants)}
> \end{align*}
> $$

最后一个式子其实就非常直观了，在穿长裤的人里面有多少穿长裤的女孩。

其他示例，
* [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
* [How to Write a Spelling Corrector](http://norvig.com/spell-correct.html)

# 随机变量和概率分布
## 随机变量
随机变量是事件的抽象，事件e到实数$X(e)$的映射，*实质上是函数*。随机变量分为离散型随机变量和连续型随机变量。

下列几个概念常常在多出见到，这里做个统一的说明。
**概率函数**表示了随机变量不同取值情况下的概率，因此也叫做**概率分布**。概率函数还叫做**概率质量函数（PMF）**。
**分布函数**表示了随机变量取值在某个范围内的概率，又叫做**累积分布函数（CDF）**，这是概率分布的*狭义*定义。 

### 离散型随机变量
**概率函数**：设 X 为离散型随机变量，$X = {a_1, a_2, \cdots}$，则 X 的概率函数为 $p_i = \Pr (X = a_i)$。
**分布函数**：函数 $\Pr (X \leq x) = F(x), -\infty < x < \infty$ 为 X 的分布函数。

对于离散型随机变量有，其 CDF 有，
* $F(x) = \Pr (X \leq x) = \sum_{i: a_i <= x}{p_i}$
* $p_i = F(i) - F(i - 1)$

对于任意随机变量 X，其 CDF 有，
* 当 $x_1 < x_2$时，$F(x_1) \leq F(x_2)$
* 当 $x \rightarrow \infty$时，$F(x) \rightarrow 1$；当 $x \rightarrow -\infty$时，$F(x) \rightarrow 0$


常用分布，
* **二项分布**
    n次独立试验，每次试验事件A发生的概率为p，事件$\{ X = i \}$试验中A发生的次数，则
    
    $$
    Pr(X = i) = b(i; n, p) = {n \choose i} p^i (1 - p)^{(n - i)}
    $$
    
    X服从的概率分布叫做二项分布，记作$X \sim B(n, p)$
* **泊松分布**
    n次独立试验，每次试验事件A发生的概率为p，事件$\{ X = i \}$试验中A发生的次数，*当n很大，p很小，$np = \lambda$不太大时*，
    
    $$
    Pr(X = i) = \frac{ e^{- \lambda} \lambda^i }{ i! }
    $$
    
    X服从的概率分布叫做泊松分布，记作$X \sim P(\lambda)$。
    
    泊松分布是对二项分布n取极限的情况，$n \rightarrow \infty$。如果对泊松分布的$i = 0, 1, 2, \dots$求和，则结果为1，因为$i = 0, 1, 2, \dots$代表了事件X发生的所有情况（定性分析）。
    
    从数学期望的角度来看（根据概率分布求期望），$\lambda$代表了时间平均发生的次数。
    
    *二项分布*和*泊松分布*有个**有意思的应用**，已知每次试验事件A发生的概率为p的情况下，**判断**试验n次事件A发生的次数（可能来自某个观测报告的数据）*是否合理*。
* **超几何分布**
    相比起二项分布，超几何分布是不放回的抽取试验。例如：对于从N个物体里面抽取的n个，若有M个废品，则抽到的物品里面包含m个废品的概率是多少。 
    
    $$
    Pr = \frac{ {M \choose m}{N - M \choose n - m} }{{N \choose n}}
    $$
    
    当*N和M远大于n*时，超几何分布近似于二项分布。
* **负二项分布**
    通过不断抽取产品，直到抽到r个废品，来预估废品率p。设事件$\{ X = i \}$为抽到的合格产品数，则该抽取试验共试验$i + r$次，为了抽到r个废品，前$i + r - 1$次中需要抽到$r - 1$个废品，第$i + r$次抽到废品，
    
    $$
    Pr(X = i) = b(r - 1; i + r - 1, p) p = {i + r - 1 \choose r - 1} p^{r - 1} (1 - p)^i
    $$
    
    由于*试验抽取的内容与二项分布相反*，因此取名负二项分布。
* **几何分布**
    当负二项分布的$r = 1$时，得到几何分布，$Pr(X = i) = p (1-p)^i$。取名几何分布，是因为公式为几何级数。

### 连续型随机变量
**概率函数**：一个连续型随机变量，可能的取值范围是一个*区间*，值的个数为*无数个*，描述取某个值的概率无意义，$\Pr (X = x) = 0$，但$X = x$*不是*不可能事件。
**累积分布函数（CDF）**：函数 $\Pr (X \leq x) = F(x), -\infty < x < \infty$ 为 X 的分布函数。
**概率密度函数（PDF）**：$f(x) = F^{\prime }(x)$。

定义概率密度函数的目的是为了便于计算连续型随机变量取值在某个范围时的概率情况。

每个概率密度函数都有，
* $f(x) \geq 0$
* $\int_{-\infty}^{\infty}f(x){\rm {d}}x=1$
* $\Pr (a\leq X \leq b) = F(b) - F(a) = \int_{a}^{b} f(x){\rm {d}}x$

常用分布，
* **正态分布**
    正态分布描述了一般事物所处的状态，“两边低，中间高”。概率密度函数，
    
    $$
    f(x) = (\sqrt{2 \pi} \delta)^{-1} e^{\frac{-(x - \mu)^2} {2 \theta^2}}, -\infty < x < \infty
    $$
    
    其中$\mu$是均值，$\theta^2$是方差。
    
    称X为正态随机变量，记作$X \sim N(\mu, \theta^2)$，$N(0, 1)$称为标准正态分布。可将任意正态分布转换为标准正态分布，$Y = (X - \mu)/\theta \sim N(0, 1)$。
* **卡方分布**
    正态分布导出的分布。$\chi^2$
* **t分布**
    正态分布导出的分布。
* **指数分布**
    指数分布描述了假定失效率为$\lambda$的情况下，元器件无老化时的寿命分布。概率密度函数，
    
    $$
    f(x) = \left \{
    {\begin{matrix}
    \lambda e^{-\lambda x} & \ \ \ {\mbox{for }} x > 0\\
    0 & {\mbox{for }} x \leq 0
    \end{matrix}}
    \right.
    $$
    
    称X服从指数分布。 
    
    变量X的分布函数$F(X)$（元器件寿命$\leq X$的概率）的推导比较有意思（《概率论与数理统计》），首先构造了一个平均失效率，$\lambda = P(m \leq X \leq m + h | X \gt m) / h$，直到m时刻正常工作的情况下，元器件寿命在$(m, m + h)$内的概率为$P(m \leq X \leq m + h | X \gt m)$，也就是在$(m, m + h)$内失效概率，除以h就是平均失效的概率，即失效率，然后令$h \rightarrow 0$，即可表示X在m处的瞬时失效率。然后根据条件概率得到微分方程，解得$F(X)$。
* **威布尔分布**
    指数分布的扩展，考虑老化，失效率$\lambda$随时间而增加的情况下，元器件的寿命分布。
* **均匀分布**
    $$
    f(x) = \left \{
    {\begin{matrix}
    {\frac  {1}{b-a}} & \ \ \ {\mbox{for }}a\leq x\leq b\\
    0 & {\mbox{elsewhere}}
    \end{matrix}}
    \right.
    $$
    
    称X为服从$[a, b]$上的均匀分布，记作$X \sim R(a, b)$
    
## 随机向量
如果n维向量$X = (X_1, X_2, \cdots, X_n)$的每个分量都是一维随机变量，那么X是一个n维随机向量。
    
### 离散型随机向量
**概率函数**：$X_i$全部可能的$\{ a_{i1}, a_{i2}, \cdots \}$取值里面，随机向量某一个特定的取值的概率为随机向量X的概率函数，记为$pr(j_1, j_2, \cdots, j_n)$。
    
常用分布，
* **多项分布**
    某实验的完备事件群$A_1, A_2, \cdots, A_n$，该实验重复N次，记$X_i$为$A_i$出现的次数，则$X = (X_1, X_2, \cdots, X_n)$为n维随机向量，记$A_n$发生的概率为$p_n$，X的概率分布叫做多项分布，记为$M(N, p_1, p_2, \cdots, p_n)$。
    
    事件群中的每个事件发生特定次数，记该事件为$B = \{ X_1 = k_1, \cdots, X_n = k_n \}$，其中$k_1 + k_2 + \cdots + k_n = N$，其概率为，
    
    $$
    Pr(X_1 = k_1, \cdots, X_n = k_n) = \frac{N!}{k_1! k_2! \cdots k_n!} p_1^{k_1} p_2^{k_2} \cdots p_n^{k_n}
    $$
    
    这里推导的思想有点像二项分布。
    
    当事件群中只有两个事件时，有$k_1 + k_2 = N$，某个事件是否发生决定了另一个事件是否发生，因此可以只考虑其中一个事件X，此时就变为了二项分布（实验重复N次，事件X发生$k_1$次的概率）。
    
### 连续型随机向量
## 随机变量的函数的概率分布
# 随机变量的数字特征
随机变量的数字特征分为：刻画了其分布和刻画了取值离散程度。

## 数学期望与中位数
### 离散型随机变量
**数学期望**：随机变量X的有限个取值$a_i$和这些值对应概率$p_i$乘积的和，即以这些取值的加权（对应概率）平均，

$$
EX = \sum_{1}^{n} a_i p_i
$$

对于无限个$(a_i, p_i)$，此时X的期望为级数之和，

$$
EX = \sum_{1}^{\infty} a_i p_i
$$

等式左边必须是**绝对收敛**$\sum_{1}^{\infty} a_i p_i \lt \infty$，即，而不能只是[条件收敛](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%94%B6%E6%95%9B)。 如果只是条件收敛，那么把$(a_i, p_i)$重排序以后，会使得式子不收敛（[黎曼级数定理](https://zh.wikipedia.org/wiki/%E9%BB%8E%E6%9B%BC%E7%BA%A7%E6%95%B0%E5%AE%9A%E7%90%86)），EX用于刻画随机变量的数字特征，值不应该与计算的次序有关。

### 连续型随机变量
**数学期望**：定义类似离散型的，把概率函数的加权平均变为概率密度函数加权的积分。可以理解为离散到无穷的时候，就变为了连续型随机变量的数学期望。

$$
EX = \int_{-\infty}^{\infty} |x| f(x) dx
$$

且要求右边式子$\lt \infty$，但这里要求收敛的原因与离散型的情况不同。

### 期望的性质
### 条件数学期望（条件均值）
### 中位数
若随机变量的某个取值m使得，$P(X \leq m) = F(m) = 1/2$，则m为X或分布F(X)的中位数，m把X的分布从概率上切成了左右两半。

中位数有如下优势，
1. 中位数不受特大值的影响；
2. 中位数始终存在，但是期望不一定（例如：柯西分布）。

但期望的应用仍然大于中位数，因为，
1. 中位数不具备类似期望的性质；
2. 按照中位数的定义，中位数可能不唯一；
3. 可能并不存在一个完美的中位数满足$F(m) = 1/2$，例如：$X \in \{1, 2, 3\}$，各取值的概率分布为$\{2/7, 4/7, 1/7\}$。

## 方差与距
衡量随机变量X偏离均值的程度，$Var(X) = E(X - EX)^2$。

## 协方差与相关系数

## 大数定理和中心极限定理

### 大数定理

### 中心极限定理

# 参数估计
## 数理统计
随机误差：统计中，不仅仅存在测量导致的误差，也存在抽样误差（不同的抽样会导致结论不同）。

数理统计就是利用概率论和数学的方法，研究如何收集带有随机误差的数据，并在统计模型下分析这些数据，最后进行统计推断。


