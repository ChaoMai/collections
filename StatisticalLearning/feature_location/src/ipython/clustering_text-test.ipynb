{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: GTK3Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from re import match\n",
    "from sklearn import random_projection\n",
    "import pandas as pd\n",
    "import os  # for os.path.basename\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name='/home/chaomai/Documents/Codes/Current/feature_location/data/AllDatajEdit4.3/Corpus-jEdit4.3/Corpus-jEdit4.3CorpusTransformedStemmed.OUT'\n",
    "java_keywords_file_name = '/home/chaomai/Documents/Codes/Current/feature_location/data/java_keywords.txt'\n",
    "functions_file_name='/home/chaomai/Documents/Codes/Current/feature_location/data/AllDatajEdit4.3/Corpus-jEdit4.3/Corpus-jEdit4.3.mapping'\n",
    "query_file_name='/home/chaomai/Documents/Codes/Current/feature_location/data/AllDatajEdit4.3/Queries-jEdit4.3ShortLongDescriptionCorpusTransformedStemmed.OUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6413\n",
      "['abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch', 'char', 'class', 'const']\n",
      "['org.gjt.sp.jedit.gui.AbbrevEditor.AbbrevEditor', 'org.gjt.sp.jedit.gui.AbbrevEditor.getAbbrev', 'org.gjt.sp.jedit.gui.AbbrevEditor.setAbbrev', 'org.gjt.sp.jedit.gui.AbbrevEditor.getExpansion', 'org.gjt.sp.jedit.gui.AbbrevEditor.setExpansion', 'org.gjt.sp.jedit.gui.AbbrevEditor.getAbbrevField', 'org.gjt.sp.jedit.gui.AbbrevEditor.getBeforeCaretTextArea', 'org.gjt.sp.jedit.gui.AbbrevEditor.getAfterCaretTextArea', 'org.gjt.sp.jedit.Abbrevs.getExpandOnInput', 'org.gjt.sp.jedit.Abbrevs.setExpandOnInput']\n"
     ]
    }
   ],
   "source": [
    "file=open(file_name)\n",
    "dataset=file.readlines()\n",
    "print(len(dataset))\n",
    "\n",
    "java_keywords_file=open(java_keywords_file_name)\n",
    "keywords=java_keywords_file.readlines()\n",
    "java_keywords=[word_tokenize(k)[0] for k in keywords]\n",
    "\n",
    "functions_file=open(functions_file_name)\n",
    "functions=functions_file.readlines()\n",
    "jedit_functions=[word_tokenize(k)[0] for k in functions]\n",
    "\n",
    "print(java_keywords[:10])\n",
    "print(jedit_functions[:10])\n",
    "\n",
    "query_file=open(query_file_name)\n",
    "query=[q for q in query_file.readlines() if len(q)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_stop_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if match('[a-zA-Z]*$', token) is not None and len(token) > 2 \\\n",
    "                and token not in stop_words \\\n",
    "                and token not in java_keywords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, n_features=5000, ngram_range=(1, 1),\n",
       "         non_negative=True, norm=None, preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize_stop_stem at 0x7f2bb0b21d90>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = HashingVectorizer(n_features=5000, non_negative=True,\n",
    "                           norm=None, binary=False,\n",
    "                           tokenizer=tokenize_stop_stem, ngram_range=(1, 1))\n",
    "hasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('hashingvectorizer', HashingVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, n_features=5000, ngram_range=(1, 1),\n",
       "         non_negative=True, norm=None, preprocessor=None, stop...'tfidftransformer', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6413, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(dataset)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6413, 700)\n",
      "Explained variance of the SVD step: 95%\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=700)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X = lsa.fit_transform(X)\n",
    "print(X.shape)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6413, 700)\n",
      "Explained variance of the SVD step: 94%\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=700)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X1=coo_matrix(X)\n",
    "X1 = lsa.fit_transform(X1.tocsr())\n",
    "print(X1.shape)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = SparsePCA(n_components=700, max_iter=300, n_jobs=4)\n",
    "pca_pipe = make_pipeline(pca, Normalizer(copy=False))\n",
    "X = pca_pipe.fit_transform(X.toarray())\n",
    "print(X.shape)\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minibatchpca = MiniBatchSparsePCA(n_components=700, n_jobs=4)\n",
    "minibatchpca_pipe = make_pipeline(minibatchpca, Normalizer(copy=False))\n",
    "X = minibatchpca_pipe.fit_transform(X.toarray())\n",
    "print(X.shape)\n",
    "explained_variance = minibatchpca.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa = FeatureAgglomeration(n_clusters=700)\n",
    "fa_pipe = make_pipeline(fa, Normalizer(copy=False))\n",
    "X = fa_pipe.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=100, n_init=1,\n",
      "    n_jobs=-1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=100, init='k-means++', max_iter=100, n_init=1, verbose=False, n_jobs=-1)\n",
    "print(km)\n",
    "X1=coo_matrix(X)\n",
    "km.fit(X1.tocsr())\n",
    "print(km.labels_.shape)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=20,\n",
      "        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=False)\n",
      "(6413,)\n",
      "Silhouette Coefficient: 0.049\n"
     ]
    }
   ],
   "source": [
    "km = MiniBatchKMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "print(km)\n",
    "km.fit(X)\n",
    "print(km.labels_.shape)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=2000))\n",
    "# a measure of how tightly grouped all the data in the cluster are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = vectorizer.fit_transform(query)\n",
    "type(Q)\n",
    "Q1=Q * svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9,  9, 19,  2,  9,  9,  9, 17,  9,  9, 12,  9,  9,  9,  9,  0,\n",
       "        2,  9,  9, 13,  9,  9,  4,  9,  9,  9,  9, 11,  9,  9,  9,  9,  9,\n",
       "        9,  9,  4,  9,  9,  9,  9, 15,  9,  9,  2,  2,  9,  9,  2, 18,  9,\n",
       "        9,  4,  9,  9,  9,  9,  9,  5,  9, 13,  9,  9,  4, 10,  9, 10,  9,\n",
       "        9,  9,  9,  9,  0, 11, 11,  9, 17,  9, 12,  2,  9, 12,  9,  9,  2,\n",
       "        9,  9,  4,  9, 17,  9,  9,  9,  2,  9,  9,  2,  9,  9,  4,  9,  9,\n",
       "        9,  9,  9,  6, 17,  9,  9,  9,  9,  6,  9,  6,  6,  6,  0,  2,  9,\n",
       "       10, 10,  9,  9,  4, 10,  9,  9,  9,  9,  9,  9, 18,  9,  9,  9,  2,\n",
       "       12, 10, 10,  9,  2,  9,  9, 10,  9,  9,  3,  9,  9,  9], dtype=int32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.predict(Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: 32 31 0 29 7 24 35 44 26 22\n",
      "Cluster 1: 6 21 22 0 29 34 20 16 30 4\n",
      "Cluster 2: 0 11 7 13 14 12 3 23 10 19\n",
      "Cluster 3: 0 4 14 15 19 13 8 25 24 29\n",
      "Cluster 4: 5 0 3 15 4 11 18 22 9 37\n",
      "Cluster 5: 13 6 2 0 43 9 7 8 5 41\n",
      "Cluster 6: 3 0 4 1 16 6 12 11 13 28\n",
      "Cluster 7: 28 30 0 29 35 31 65 10 51 86\n",
      "Cluster 8: 2 0 14 9 4 11 20 10 21 35\n",
      "Cluster 9: 0 10 6 64 26 33 17 63 68 61\n",
      "Cluster 10: 1 0 7 4 5 10 15 12 21 28\n",
      "Cluster 11: 0 1 14 16 9 8 17 32 15 34\n",
      "Cluster 12: 16 0 5 20 7 28 24 10 15 30\n",
      "Cluster 13: 6 0 33 36 38 37 14 46 50 35\n",
      "Cluster 14: 0 24 26 40 57 1 17 11 112 95\n",
      "Cluster 15: 17 0 3 15 7 21 20 18 41 27\n",
      "Cluster 16: 54 55 49 62 0 57 42 38 44 46\n",
      "Cluster 17: 8 10 0 3 17 14 20 18 12 16\n",
      "Cluster 18: 31 0 38 25 17 44 27 34 46 53\n",
      "Cluster 19: 24 0 27 32 10 34 13 6 50 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(20):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % ind, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_stop_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if match('[a-zA-Z]*', token) is not None and len(token) > 2 \\\n",
    "                and token not in stop_words \\\n",
    "                and token not in java_keywords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for word in word_tokenize(text)]\n",
    "    filtered_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if match('[a-zA-Z]*', token) is not None and len(token) > 2 \\\n",
    "                and token not in stop_words \\\n",
    "                and token not in java_keywords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303038\n",
      "303038\n"
     ]
    }
   ],
   "source": [
    "totalvocab_stop_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in range(len(dataset)):\n",
    "    allwords_stemmed = tokenize_stop_stem(dataset[i])\n",
    "    totalvocab_stop_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(dataset[i])\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stop_stemmed)\n",
    "print(len(totalvocab_stop_stemmed))\n",
    "print(len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(X)\n",
    "MDS()\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e',\n",
    "                  5: '#1b9e77', 6: '#d95f02', 7: '#7570b3', 8: '#e7298a', 9: '#66a61e',\n",
    "                  10: '#1b9e77', 11: '#d95f02', 12: '#7570b3', 13: '#e7298a', 14: '#66a61e',\n",
    "                  15: '#1b9e77', 16: '#d95f02', 17: '#7570b3', 18: '#e7298a', 19: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n",
    "                 5: '0', 6: '1', 7: '2', 8: '3', 9: '4',\n",
    "                 10: '0', 11: '1', 12: '2', 13: '3', 14: '4',\n",
    "                 15: '0', 16: '1', 17: '2', 18: '3', 19: '4',\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=km.labels_.tolist(), title=functions)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
