# 6.824 - Spring 2017 Lecture 1: Introduction

# Readings

## MapReduce: Simplified Data Processing on Large Clusters

### 实现

map task运行时，

* 定期将生成的kv pair写入本地磁盘，并被parition函数分为R个分区。
* 这些在磁盘上的pair的位置会被返回给master，master进而把这些位置发给reduce task。

reduce task运行时，

* 使用RPC读取map task缓存到磁盘的数据。
* 当reduce worker读取到所有数据时，按临时key对数据排序。因为多个intermediate key可能会由同一个Reducer处理，因此需要排序使得相同的key在一起。
* reduce worker遍历已排序的中间数据，将key和中间value传给用户定义的reduce函数。

### master数据结构

* 对于每个map task和reduce task，master保存了任务状态（idle, in-progress, or completed），以及每个worker机器的身份。
* 对于每个已完成的map task，master保存了R个分区的中间数据文件的位置和大小，每当一个map task完成，中间数据文件的位置和大小就被更新。这些信息以增量的形式，发送给有正在运行reduce task任务的机器。

### 容错

由于长时间未响应master的ping，worker故障，

* 正在运行的map task或reduce task重置为idle->重跑。
* 已完成的map task重置为idle->重跑，因为map task生成的中间数据是本地存储的。
* 已完成的reduce task无需重跑，因为输出已经保存到GFS。

当一个map task先被worker A执行，接着又被B执行（worker A失败），这次重跑会被通知到所有执行reduce task的机器。任何还没有从A读取数据的reduce task，都转而向B读取。

master故障，

* 定期保存master数据结构的checkpoint。

# Lectures

## Distributed System

分布式系统提供了app使用的基础设施，通过抽象隐藏了实现的细节，这些抽象包括：

* 存储（storage）
* 通信（communication）
* 计算（computation）

在讨论分布式系统时，下面几个话题会时常出现：

* 实现
    RPC，线程，并发控制。
    
* 性能
    * 理想情况：可扩展的吞吐量。N倍的服务器数量->（通过并行的CPU，磁盘，网络实现）N倍的吞吐量。因此为了处理更多的负载，只需要添加更多的机器。
    * 可扩展性随着N增加而变得困难：
        * 负载不均衡（Load im-blance），集群里有慢的机器（stragglers）。
        * 不可并行的代码：初始化，交互
        * 共享资源的瓶颈：网络
* 容错
    * 大集群，复杂的网络->总会有出问题的地方
    * 希望能够隐藏这些错误，使得错误对app不可见
        * 可用性（Availability）：在出错的情况下，app能继续使用数据。
        * 持久性（Durability）：修复错误后，app能够继续正常工作。
    * big idea：多个服务器。如果一个server crash，client能继续使用其他的。
* 一致性（consistency）
    * 通用目的的架构需要有良好定义的行为。
        * 例如：`get(k)`应该返回最近的`put(k, v)`。
    * 实现良好定义的行为时困难的。
        * 难以保证多个服务器一致。
        * 在含有多个操作的更新中，客户端执行到一半可能crash了。
        * 服务器在尴尬的时刻崩溃，例如：执行了操作但没有返回结果。
        * 由于网络问题，服务器看起来好像挂了。
    * 一致性和性能是冲突的。
        * 实现一致性需要通信。
        * “强一致性”常常导致系统性能低下。
        * 高性能通常会对app造成“弱一致性”。

## MapReduce

### MapReduce概述

* 上下文：对海量数据进行多个小时的运算。
* 总目标：普通程序员能够在保证合理的效率的情况下，轻松的将数据处理切分到多个服务器。
* 需要定义Map和Reduce函数。
* MR的运行：集群，海量数据，隐藏分布式的细节。

```
input is divided into M files
Input1 -> Map -> a,1 b,1 c,1
Input2 -> Map ->     b,1
Input3 -> Map -> a,1     c,1
              |   |   |
              |   |   -> Reduce -> c,2
              |   -----> Reduce -> b,2
              ---------> Reduce -> a,2
```

### MapReduce隐藏了很多蛋疼的细节

* 在服务器上启动s/w
* 跟踪已结束的任务
* 数据移动
* 错误恢复

### MapReduce有很好的可扩展性

N台机器->N倍吞吐量。假设M和R都>=R（大量的输入和输出文件）。由于`Map()`之间无交互，`Reduce()`也是，都能够并行执行，因此可以通过添加机器来增加吞吐量。

### 性能受限的潜在因素

网络。在Map->Reduce的shuffle时，所有数据都需要通过网络传输，因此减少通过网络来移动的数据是关键。

### 更多细节

* mater：为worker分配任务，记录中间输出的位置。
* M Map tasks, R Reduce tasks。
* 输入存储在GFS，3份冗余。
* 集群所有机器都运行GFS和MR worker。
* input task比workder数量更多。
* mater为每个worker分配Map task。
* Map worker将中间key hash到R个partition，只有当所有Map结束后，本地的Reduce调用才会开始。
* mater告诉Reducer从Map worker获取中间数据partition（intermediate data partitions）。
* Reduce workers将最终结果写入GFS（一个文件对应一个Reduce task）。

### 设计细节：减少低速网络的影响

* Map从本地读入GFS上的数据备份（replica），而不是从网络。
* 中间数据仅仅在网络中传输一次。Map worker将数据写入本地磁盘，而不是GFS。
* 中间数据被partition到包含很多key的文件。大网络中的传输更加高效。

### 如何实现良好的load balance？

load balance对于可扩展性很重要，N-1个server都等待1个server结束是不好的。但是有的任务就是会比其他花费更长的时间。

解决方案：任务数比worker数更多。
master为已完成当前任务的worker分配新的任务。一般来说，就不会有大任务主宰计算时间。那么快的server会比慢的server完成更多的任务，最终同时完成所有任务。

### 容错？

如果某个server在执行MR任务期间crash怎么办？
不是重启整个job，MR只重新运行失败的`Map()`的`Reduce()`。这两个操作必须是纯函数：

* 不保留调用之间的状态。
* 不读取和写入除了MR输入/输入的文件。
* 任务之间没有隐藏的通信。

因此重新执行会生成相同的结果。纯函数的要求是MR相比于其他并发编程模型的主要局限，当它也是MR简洁的关键。

### crash recovery的细节

* Map worker crashes：
    * master观察到worker再也不响应ping。
    * crash的worker的中间Map输出丢失，当这个数据有可能每个Reduce任务都需要。
    * master根据GFS上输入数据的其他备份来分配任务，并重新执行。
    * 某些Reduce worker可能已经读取了crash的worker生成的中间数据。此时就需要依赖于`Map()`的纯函数特性和确定性。
    * 如果Reduce获取到了所有的中间数据，那么master就不需要重新运行Map。虽然接下来的一个Reduce会crash，进而导致强制重跑失败的Map。
* Reduce worker crashes：
    * 已结束的任务不受影响，数据以冗余的形式存储在GFS。master重新执行其他worker上未完成的任务。
* Reduce worker在写入输出数据期间crash：
    * GFS的rename是atomic的，在写入完成前数据不可见。因此master可以安全的在其他地方重跑Reduce任务。

### 其他错误和问题

* master给两个worker分配了相同的`Map()`任务怎么办？
    原因可能是master错误的认为worker挂了。master只会将其中一个告诉给Reduce worker。
    
* master给两个worker分配了相同的`Reduce()`任务怎么办？
    两个都会尝试在GFS中写入相同的文件。GFS rename的atomic性质避免了结果是两者的混合，只有一个完整的文件可见。
    
* 如果某个worker很慢怎么办？
    原因可能是硬件问题。master重新运行最后几个任务。
    
* 如果worker由于h/w或s/w问题计算出了错误的输出怎么办？
    ╮(╯_╰)╭，MR假设CPU和software是“[fail-stop](https://www.cs.cornell.edu/fbs/publications/FailStop.pdf)”的。

* 如果master crash了怎么办？
    *（原文并没有给出答案）*使用多个master，一个可用，剩余standby。
    

### 什么样的app不适用于MapReduce？

* 不是所有app都适用于map/shuffle/reduce模式。
* 小数据量，因为开销很高。例如：网站的后端。
* 对海量数据的小更新，因为Map和Reduce都不能选择输入数据的大小。
* 多个shuffle过程，因为会导致多个MR，不高效。
* 更灵活的系统可以实现上述目标，但会导致更复杂的模型。

### 结论

MapReduce让集群计算受欢迎。

* 不是最高效和灵活的。
* 可扩展性良好。
* 易于编程，错误和数据移动被隐藏了。

这些在实践中是很好的权衡。

## Lab 1: MapReduce

* 实验给出了框架代码，需要完成关键函数。
* 要求用Go。
* MapReduce的分布式实现除了需要关注task，还需要考虑存储，分布式存储不是这里的重点，因此实验在一台机器上运行worker thread，使用系统的文件系统模拟分布式存储。
* 实验要求实现两种模式的MR，顺序执行所有task，以及分别并行执行map和reduce task。实验给出了的框架代码使用了go的channel来实现并行执行task。而对于顺序执行task的实现，实际上是分别把所有map和reduce task做了一次封装，得到“一个map task”和“一个reduce task”，分别执行封装好的“map task”和“reduce task”，其中每个实际的map和reduce都是顺序执行的。

### Part I: Map/Reduce input and output

实现`doMap()`和`doReduce()`，并顺序执行所有task。


