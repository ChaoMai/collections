# 线性回归及其相关算法

# 线性回归（linear regression）

给定由 $d$ 个属性描述的样本 $\mathbf{x} = (x_{i1}; x_{i2}; ...; x_{id})$，**线性模型**试图学得一个通过属性的线性组合来进行预测的函数，

$$
f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = \mathbf{w}^{\top} \mathbf{x} + b
$$

其中，$\mathbf{w} = ((w_1; w_2; ...; w_d))$。

给定数据集 $D = {(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), ..., (\mathbf{x}_n, y_n)}$，其中 $\mathbf{x} = (x_{i1}, x_{i2}, ..., x_{id}), y_i \in \mathbb{R}$，

线性回归试图学得一个**线性模型**，$f(\mathbf{x}_i) = \mathbf{w}^{\top} \mathbf{x}_i + b$，使得 $f(\mathbf{x}_i) \approx y_i$。

可用最小二乘来求解 $\mathbf{w}$ 和 $b$。有可能会得到多个 $\mathbf{w}$ 和 $b$，使得均方误差最小。此时选择哪一个，依赖于算法的归纳偏好，常见做法是引入正则化项。亦可使用梯度下降求解。

为简便，将线性回归模型记作，

$$
y = \mathbf{w}^{\top} \mathbf{x} + b
\tag{1}
$$

将线性模型预测试逼近 $\operatorname{ln} y$，即 $\operatorname{ln} y = \mathbf{w}^{\top} \mathbf{x} + b$，那么可以得到“对数线性回归”，这实际上就是试图让 $e^{\mathbf{w}^{\top} \mathbf{x} + b}$ 逼近 $y$，对数函数起到了“联系函数”的作用。

考虑一般的**“联系函数”**，一个单调可微函数 $g(\cdot)$，令

$$
y = g^{-1}(\mathbf{w}^{\top} \mathbf{x} + b)
\tag{2}
$$

这便得到了**“广义线性模型”**。


# 对数几率回归（logistic regression）

线性回归不适用于二分类问题，需要使用广义线性模型，找到一个单调可微函数，将线性回归模型和标记 $y$ 联系起来，这个函数是对数几率函数，

$$
y = \frac{1}{1 + e^{-z}}
\tag{3}
$$

此函数是“Sigmoid 函数”（形似 S 的函数）的一种。将 $(3)$ 作为 $g(\cdot)$ 带入 $(2)$ 可得，

$$
y = \frac{1}{1 + e^{-(\mathbf{w}^{\top} \mathbf{x} + b)}}
\tag{4}
\\
\operatorname{ln} \frac{y}{1 - y} = \mathbf{w}^{\top} \mathbf{x} + b
$$

$\frac{y}{1 - y}$ 叫**“几率”**，反映了 $\mathbf{x}$ 作为正例的可能性。$\operatorname{ln} \frac{y}{1 - y}$ 叫**“对数几率”**。$(4)$ 将线性模型预测试逼近真实标记的对数几率，对应的模型叫做**“对数几率回归”**。


## 广义线性模型



# Reference

1. [对数线性模型之一(逻辑回归), 广义线性模型学习总结](http://blog.csdn.net/lilyth_lilyth/article/details/10032993)
2. [线性回归](http://www.cnblogs.com/dreamvibe/p/4244812.html)
3. [广义线性模型（GLM）](http://www.cnblogs.com/dreamvibe/p/4259460.html)
4. [最大似然估计(Maximum likelihood estimation)](http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html)
5. [CS 229 Machine Learning Course Materials](http://cs229.stanford.edu/materials.html)

