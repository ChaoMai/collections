This is the sequence that I do: 
1. Put as much data in memory as I can to reasonably estimate reality 
(1/10, 1/100, 1/2, etc., pick one), then multiply it out to get the 
"true" number 
2. Add 20% for memory fragmentation or data inconsistency. 
3. Calculate my number of operations per second and how many keys are 
volatile (have expiration times) during the rewrite 
4. Calculate a worst-case increase in memory consumption at 4k * 
(ops/s + volatile keys) 
5. If the increase in memory consumption is > the memory use + 
fragmentation, then call it double. If it's less, then use the lesser 
number 
6. If slaving is going to be done, calculate the size of the dump for 
initial slaving, and multiply it out if you will have more than one 
slave, and add that in 
7. If your network is not fast, and sync times are large, you may need 
to add in some memory for batched up commands during sync 
8. If pubsub will be used, multiply the buffer size limit for pubsub 
times the expected number of clients, and add that in 


For example: 
1. 4G 
2. + 800M 
3. dump takes 5 seconds, producing a 400M dump, at 10k writes/second, 
20k volatile keys... 
4. + (5 * 10k + 20k) * 4k = 280M 
5. Not double, use 280M 
6. 2 slaves, so + 2 * 400M 
7. Fast network, not applicable 
8. 50 pubsub clients, 8M each so +400M 

Total: 4G(1) + 800M(2) + 280M(4) + 800M(6) + 400M(8) = 6.28G estimated 
required memory, including rewrite/pubsub overhead, and key 
modifications


