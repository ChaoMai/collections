# 1. 行列式

### 1.1 定义

有[三种定义方式](https://www.zhihu.com/question/26294660/answer/32525286)，

* 排列和逆序数
* 用代数余子式和按行展开来递归定义
* 用归一化、多线性、反对称来定义
    1. 归一化（单位矩阵行列式为1）、多线性（当矩阵的某一列所有元素都扩大c倍时，相应行列式也扩大c倍。多的意思是对所有n个列都呈现线性性质）、反对称（交换两列行列式反号）
    2. 行列式等于它的各个列对应的向量张成的平行2n面体的体积，这是因为行列式是一个交替多重线性形式，而我们通常理解的欧式空间中的体积也是这样一个函数（单位立方体体积为1，沿某条边扩大c倍体积就扩大c倍，交换两条边以后体积反号——这一条是补充定义的，我们认为体积是有向体积，其数值表示体积大小，正负号表示各条边的排列顺序或坐标轴手性），而满足归一性、多线性、反对称性的函数是唯一的，所以行列式的直观理解就是欧式空间中的有向体积

### 1.2 矩阵角度的几何意义

$2 \times 2$矩阵的行列式定义如下：

$$
\det(A)=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}=
ad-bc
$$

矩阵的两个行向量构成了一个平行四边形，$ad-bc$的绝对值是这个平行四边形的面积。$3 \times 3$矩阵的行列式的值是这个三个行向量构成的平行四面体的体积的绝对值，同理可扩展到n阶矩阵的行列式。

对于一个$n \times n$的行列式，如果其中的一个行向量位于剩下的行向量构成的超平面上，即线性相关，那么$\det(A)=0$。

### 1.3 线性变换角度的几何意义

矩阵对应的线性变换对空间的拉伸程度的度量，或者说物体经过变换前后的**体积比**。

如果矩阵不是满秩的，那么在线性变换后，一个$n$维的空间被“压到了”$k$维，或者说被*压扁了*，原来空间中的体积在变换后体积为0。

# 2. 矩阵

## 2.1 矩阵与线性变换

$$
\begin{cases}
y_1 = a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n, \\
y_2 = a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n, \\
\cdots \\
y_m = a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n, \\
\end{cases}
$$

上述方程组，表示从变量$x_1, x_2, \cdots, x_n$到变量$y_1, y_2, \cdots,  y_m$的线性变换，其中系数$a_{ij}$构成一个矩阵。

给定了一个线性变换，它的系数构成的矩阵也就确定，反之，如果给出了一个矩阵，则线性变换也就确定，两者之间存在一一对应的关系。

证明如下（取自[Proving any linear transformation can be represented as a matrix](http://math.stackexchange.com/questions/916192/proving-any-linear-transformation-can-be-represented-as-a-matrix)）：
Consider a matrix $\mathbf x \in \mathbb R^n$ given by
$$
\begin{align*} \mathbf x &= \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}. \end{align*}
$$
We will construct a matrix $A \in \mathbb R^{n \times n}$ such that $T(\mathbf x) = A \mathbf x$.

The vector $\mathbf x$ can also be written as
$$
\begin{align*} \mathbf x &= x_1 \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} + x_2 \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix} + \dotsb + x_n \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix} \\ &= x_1 \mathbf{e}_{1} + x_2 \mathbf{e}_{2} + \dotsb + x_n \mathbf{e}_{n} \\ &= \sum_{i=1}^{n} x_i \mathbf{e}_{i}, \end{align*}
$$
where $\mathbf{e}_{i}$ are the standard basis vectors in $\mathbb R^n$.

Consider the transformation $T(\mathbf x)$. Rewriting $\mathbf x$ as above, we have
$$
\begin{align} T(\mathbf x) &= T \left( \sum_{i=1}^{n} x_i \mathbf{e}_{i} \right) \\ &= \sum_{i=1}^{n} T(x_i \mathbf{e}_{i}) \\
T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{1} \end{align}
$$

Let the matrix $A \in \mathbb R^{n \times n}$ be defined by
$$
\begin{align*} A &= \begin{bmatrix} T(\mathbf{e}_{1}) & T(\mathbf{e}_{2}) & \cdots & T(\mathbf{e}_{n}) & \end{bmatrix} \\ &= \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{bmatrix}, \end{align*}
$$
where each $T(\mathbf{e}_{i})$ is a column of $A$, and each $a_{ij} = T(\mathbf{e}_{i}) \cdot \mathbf{e}_{j}$ is the $j$th component of $T(\mathbf{e}_{i})$. Then, by the definition of matrix-vector multiplication, we have
$$
\begin{align*} A \mathbf x &= \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{bmatrix} \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \\ &= \begin{bmatrix} x_1 a_{11} + \dotsb + x_n a_{1n} \\ \vdots \\ x_1 a_{n1} + \dotsb + x_n a_{nn} \\ \end{bmatrix} \\ &= x_1 \begin{bmatrix} a_{11} \\ \vdots \\ a_{n1} \end{bmatrix} + \dotsb + x_n \begin{bmatrix} a_{n1} \\ \vdots \\ a_{nn} \end{bmatrix} \\ &= x_1 T(\mathbf{e}_{1}) + \dotsb + x_n T(\mathbf{e}_{n}) \\
A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{2} \end{align*}
$$

Therefore, by eqs. (1) and (2), we have that
$$
\begin{align*} T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}) & A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}), \end{align*}
$$
and we reach $T(\mathbf x) = A \mathbf x$, as was to be shown.

## 2.2 矩阵乘法

一个$m \times s$矩阵和一个$s \times n$矩阵的乘积可以看作两个线性变换合并的结果，例如有下面两个线性变换，

$$
\begin{cases}
y_1 = b_{11} x_1 + b_{12} x_2 + \cdots + b_{1s} x_s, \\
y_2 = b_{21} x_1 + b_{22} x_2 + \cdots + b_{2s} x_s, \\
\cdots \\
y_m = b_{m1} x_1 + b_{m2} x_2 + \cdots + b_{ms} x_s, \\
\end{cases} \tag{1}
$$

$$
\begin{cases}
x_1 = a_{11} t_1 + \cdots + a_{1n} t_n, \\
x_2 = a_{21} t_1 + \cdots + a_{2n} t_n, \\
\cdots \\
x_s = a_{s1} t_1 + \cdots + a_{sn} t_n, \\
\end{cases} \tag{2}
$$

将$(2)$带入$(1)$可以有，

$$
\begin{cases}
\begin{aligned}
y_1 = {} & (a_{11}b_{11} + a_{21}b_{11} + \cdots + a_{s1}b_{11}) t_1 + \\
& (a_{11}b_{12} + a_{21}b_{12} + \cdots + a_{s1}b_{12}) t_2 + \\
& \cdots + \\
& (a_{11}b_{1s} + a_{21}b_{1s} + \cdots + a_{s1}b_{1s}) t_n, \\
y_2 = {} & \cdots  \\
\vdots \\
y_m = {} & (a_{11}b_{m1} + a_{21}b_{m1} + \cdots + a_{s1}b_{m1}) t_1 + \\
& (a_{11}b_{m2} + a_{21}b_{m2} + \cdots + a_{s1}b_{m2}) t_2 + \\
& \cdots + \\
& (a_{11}b_{1m} + a_{21}b_{ms} + \cdots + a_{s1}b_{ms}) t_n, \\
\end{aligned}
\end{cases}
$$

即，$Y = BX$

## 2.3 伴随矩阵

$\operatorname{adj}(A)$，矩阵$A$的余子矩阵的转置，即，

$$
\operatorname{adj}(A) =
\begin{bmatrix}
A_{11} & A_{21} & \cdots & A_{n1} \\
A_{12} & A_{22} & \cdots & A_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
A_{1n} & A_{2n} & \cdots & A_{nn} \\
\end{bmatrix}
$$

伴随矩阵有以下性质，$A \operatorname{adj}(A) = \operatorname{adj}(A) A = \det(A) I$

上述性质的证明可见[proof 1](http://math.stackexchange.com/questions/1404189/why-in-the-proof-of-a-cdot-adja-deta-cdot-i-n-entires-not-on-the-diagonal)和[proof 2](http://math.stackexchange.com/questions/345517/why-is-it-true-that-mathrmadjaa-deta-cdot-i)，简单说明如下，

考虑一个$3 \times 3$的矩阵，

$$
A \operatorname{adj}(A) =
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
A_{11} & A_{21} & A_{31} \\
A_{12} & A_{22} & A_{32} \\
A_{13} & A_{23} & A_{33} \\
\end{bmatrix}
$$

可得，

$$
\begin{bmatrix}
\underbrace{a_{11}A_{11} + a_{12}A_{12} + a_{13}A_{13}}_{(1)} & \underbrace{a_{21}A_{11} + a_{22}A_{12} + a_{23}A_{13}}_{(2)} & \cdots \\
\cdots & a_{21}A_{21} + a_{22}A_{22} + a_{23}A_{23} & \cdots \\
\cdots & \cdots & a_{31}A_{31} + a_{32}A_{32} + a_{33}A_{33} \\
\end{bmatrix}
$$

其中，

$$
\begin{aligned}
& (1) = a_{11}A_{11} + a_{12}A_{12} + a_{13}A_{13} =
\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{vmatrix} =
\det(A) \\
& (2) = a_{21}A_{11} + a_{22}A_{12} + a_{23}A_{13} =
\begin{vmatrix}
a_{21} & a_{22} & a_{23} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{vmatrix} =
0 \\
\end{aligned}
$$

对于剩下的式子，同理可得类似的结果。

## 2.4 逆矩阵

对于线性变换$Y=AX \tag{1}$

$$
\begin{aligned}
& \operatorname{adj}(A) Y = \operatorname{adj}(A) AX \\
& \operatorname{adj}(A) Y = \det(A) X \\
& X = \frac{1}{\det(A)} \operatorname{adj}(A) Y \\
\end{aligned}
$$

设，$B = \frac{1}{\det(A)} \operatorname{adj}(A)$，则有$X = BY \tag{2}$

将$(2)$带入$(1)$可得，$Y = A(BY) = (AB)Y$，将$(1)$带入$(2)$可得，$X = B(AX) = (BA)X$，因此$BA = AB = I$，$B$是$A$的逆矩阵。

当$\det(A) = 0$时，$A$成为奇异矩阵，否则称非奇异矩阵。

## 2.5 向量和矩阵的乘法

### 2.5.1 点积（dot product）或标量积（scalar product）

两个向量$\mathbf{a} = [a_1, a_2, \cdots, a_n]$和$\mathbf{b} = [b_1, b_2, \cdots, b_n]$

#### 2.5.1.1 代数定义

$$
\mathbf{a} \cdot \mathbf{b} = \sum^n_{i=1} a_i b_i = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n
$$

#### 2.5.1.2 几何定义

$$
\mathbf{a} \cdot \mathbf{b} = \lVert \mathbf{a} \lVert \lVert \mathbf{b} \lVert \cos(\theta)
$$

### 2.5.2 内积（inner product）

内积把点积泛化到了域或标量上的抽象向量空间。

### 2.5.3 外积（outer product）

两个坐标向量的张量积叫做外积，是矩阵的Kronecker积的特殊形式。

$$
\mathbf{u} \otimes \mathbf{v} =
\begin{bmatrix}
u_1 \\
u_2 \\
u_3 \\
u_4 \\
\end{bmatrix}
\begin{bmatrix}
v_1 & v_2 & v_3
\end{bmatrix} =
\begin{bmatrix}
u_1 v_1 & u_1 v_2 & u_1 v_3 \\
u_2 v_1 & u_2 v_2 & u_2 v_3 \\
u_3 v_1 & u_3 v_2 & u_3 v_3 \\
u_4 v_1 & u_4 v_2 & u_4 v_3 \\
\end{bmatrix}
$$

#### 2.5.3.1 Kronecker积

两个矩阵$\mathbf{A} \in \mathbb{R}_{I \times J}$和$\mathbf{B} \in \mathbb{R}_{K \times L}$的Kronecker积记作$\mathbf{A} \otimes \mathbf{B}$，乘积的大小是$(IK) \times (JL)$，结果是，

$$
\mathbf{A} \otimes \mathbf{B} =
\begin{bmatrix}
a_{11} \mathbf{B} & a_{21} \mathbf{B} & \cdots & a_{1J} \mathbf{B} \\
a_{21} \mathbf{B} & a_{22} \mathbf{B} & \cdots & a_{2J} \mathbf{B} \\
\vdots & \vdots & \ddots & \vdots \\
a_{I1} \mathbf{B} & a_{I2} \mathbf{B} & \cdots & a_{IJ} \mathbf{B} \\
\end{bmatrix} =
\begin{bmatrix}
\mathbf{a}_1 \otimes \mathbf{b}_1 & \mathbf{a}_1 \otimes \mathbf{b}_2 & \mathbf{a}_1 \otimes \mathbf{b}_3 & \cdots & \mathbf{a}_J \otimes \mathbf{b}_{L-1} & \mathbf{a}_J \otimes \mathbf{b}_L
\end{bmatrix}
$$

相当于矩阵$\mathbf{A}$的每个元素都与$\mathbf{B}$相乘。

# 3. 初等矩阵

## 3.1 初等变换

一个单位矩阵经过一次初等变换，得到初等矩阵。

* Row switching：$R_i \leftrightarrow R_j$
* Row multiplication：$kR_i \rightarrow R_i, where k \neq 0$
* Row addition：$R_i + kR_j \rightarrow R_i, where i \neq j$

以上三种初等行（列）变换，统称初等变换。初等变换有，反身性，对称性和传递性。

设$A$是一个$m \times n$的矩阵，存在下述性质，

* 对$A$施行一次初等*行变换*，相当于对$A$*左乘*一个相应的$m$阶初等矩阵；对$A$施行一次初等*列变换*，相当于对$A$*右乘*一个相应的$m$阶初等矩阵。
* 方阵$A$可逆的充分必要条件是存在有限个初等矩阵$P_1, P_2, \cdots, P_t$，使得$A = P_1 P_2 \cdots P_t$。

## 3.2 矩阵的秩

### 3.2.1 标准形

一个$m \times n$的矩阵$A$，总可以经过初等变换，变为一个左上角是单位矩阵的矩阵$B$，矩阵$B$就叫做标准形。

$$
B = 
\begin{bmatrix}
E_r & O \\
O & O \\
\end{bmatrix}
$$

矩阵$B$的大小为$m \times n$，$r$是非零行的行数，这个$r$是唯一的。$m, n, r$可以确定一个标准形。

标准形的非零行的行数$r$，是矩阵的秩，记为$\operatorname{R}(A)$。

观察标准形，可发现，那些非零行其实是线性无关的(可以作为空间的基底)，由此引出另外两种秩的定义，

* 一个矩阵$A$的列秩是$A$的线性无关的列的极大数目。类似地，行秩是A的线性无关的行的极大数目，矩阵的列秩等于行秩。
* 一个矩阵$A$的秩是由它的行（列）向量生成的行（列）空间的维度（参考[Rank (linear algebra)](https://en.wikipedia.org/wiki/Rank_(linear_algebra))）。

一个秩为一的矩阵可以写作两个非零列向量的外积，$A = \mathbf{u} \otimes \mathbf{v}$。乘积结果的每个行向量都是向量$\mathbf{v}^{\top}$乘以一个常数，全都线性相关。

一般来说，一个矩阵的秩就是构成这样外积的和的最小的个数，从另一个角度来说，每个矩阵都可以表示为多个秩一矩阵的和的形式。

### 3.2.2 $k$阶子式和秩

由矩阵$A$的$k$行$k$列交叉处的$k^2$个元素组成的行列式，叫做矩阵$A$的$k$阶子式。

对于矩阵$A$的最高阶非零子式，其阶数$k$，就是矩阵$A$的秩。

$n$阶可逆矩阵$A$的$\det(A) \neq 0$，因此$\operatorname{R}(A) = n$，可逆矩阵又叫做满秩矩阵，奇异矩阵又叫做降秩矩阵。

# 4. 向量空间（Vector Space）

## 4.1 向量空间

A vector space $V$ over a filed $\mathbb{F}$ a set that is closed under **vector addition** and **scalar multiplication**. 

如果域$\mathbb{F}$是实数域$\mathbf{R}$，那么向量空间叫做实数向量空间；如果是复数域$\mathbf{C}$，那么叫做复数向量空间。

性质：

若$a, b \in \mathbb{R}$，$\mathbf{u}, \mathbf{v} \in V$，

* $\mathbf{u} + \mathbf{v} = \mathbf{w}$，则$\mathbf{w} \in V$
* $a \mathbf{u} \in V$
* $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$

## 4.2 线性无关（Linearly Independent）

### 4.2.1 线性组合

设$V$为向量空间，$\mathbf{R}$为实数域，给定$A =\{ \mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_m} \} \subset V$和$k_1, k_2, \cdots, k_m \in \mathbf{R}$，$k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m}$叫做$A$的一个线性组合。再给定向量$\mathbf{b} \in B$，若有$\mathbf{b} = k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m}$，则向量$\mathbf{b}$能由$A$线性表示，也意味着以那组实数为未知数的方程有解。

### 4.2.2 线性无关（Linearly Independent）

设$V$为向量空间，$\mathbf{R}$为实数域，给定$A =\{ \mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_m} \} \subset V$和一组不全为零的实数$k_1, k_2, \cdots, k_m$，使得$k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m} = \mathbf{0}$，则$A$线性相关，否则线性无关。

线性相关的几何意义在于$A$的所有向量位于同一平面。若$m \geq 2$，则说明$A$中，至少有一个向量能由其余的$m - 1$个向量线性表示，即

$$
\mathbf{a_1} = -\frac{1}{k_1} (k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m})
$$

给定向量空间$V$，若能选出r个向量$V_0: \mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_m}$，满足

1. $\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r}$线性无关；
2. $\forall \mathbf{u} \in A$, $\mathbf{u} = k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m}$, where $k_1, k_2, \cdots, k_n \in \mathbf{R}$

则$V_0$是$V$的一个maximal linearly independent subset，$V$的秩是其maximal linearly independent subset所含向量的个数$r$，maximal linearly independent subset一般**不唯一**。

## 4.3 基和维度

### 4.3.1 基和维度（Basis and Dimension）

设$V$为向量空间，如果r个向量$\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r} \in V$，且

1. $\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r}$线性无关；
2. $\forall \mathbf{u} \in V$, $\mathbf{u} = k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m}$, where $k_1, k_2, \cdots, k_n \in \mathbf{R}$

那么$\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r}$就称为向量空间$V$的**一个基**，$r$称为向量空间$V$的维数，称$V$为$r$维向量空间。

如果把$V$看作向量组，则$V$的基就是maximal linearly independent subset，$V$的维数就是秩。

在向量空间$V$取一个基$\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r}$就称为，那么

$\forall \mathbf{u} \in V$, $\mathbf{u} = k_1 \mathbf{a_1} + k_2 \mathbf{a_2} + \cdots + k_m \mathbf{a_m}$, where $k_1, k_2, \cdots, k_n \in \mathbf{R}$，

$k_1, k_2, \cdots, k_n$叫做向量$\mathbf{u}$在基$\mathbf{a_1}, \mathbf{a_2}, \cdots, \mathbf{a_r}$中的坐标。

### 4.3.2 相同维度下的基变换和坐标变换

在$V: \mathbb R^3$中取两个基$\mathbf{A} = (\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3})$和$\mathbf{B} = (\mathbf{b_1}, \mathbf{b_2}, \mathbf{b_3})$，基变换的推导如下，

$$
\begin{aligned}
& (\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}) = (\mathbf{e_1}, \mathbf{e_2}, \mathbf{e_3}) \mathbf{A} \\
& (\mathbf{e_1}, \mathbf{e_2}, \mathbf{e_3}) = (\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}) \mathbf{A}^{-1} \\
\end{aligned}
$$

可得，$(\mathbf{b_1}, \mathbf{b_2}, \mathbf{b_3}) = (\mathbf{e_1}, \mathbf{e_2}, \mathbf{e_3}) \mathbf{B} = (\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}) \mathbf{A}^{-1} \mathbf{B}$

因此基变换公式为，$(\mathbf{b_1}, \mathbf{b_2}, \mathbf{b_3}) = (\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}) \mathbf{P}, \text{其中}\mathbf{P} = \mathbf{A}^{-1} \mathbf{B}$。

设向量$\mathbf{x}$在$\mathbf{A}$和$\mathbf{B}$中的坐标分别为$m_1, m_2, m_3$和$n_1, n_2, n_3$，即

$$
\mathbf{x} =
(\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}) \begin{bmatrix}
m_1 \\
m_2 \\
m_3 \\
\end{bmatrix} =
(\mathbf{b_1}, \mathbf{b_2}, \mathbf{b_3}) \begin{bmatrix}
n_1 \\
n_2 \\
n_3 \\
\end{bmatrix}
$$

所以有，

$$
\mathbf{A} \begin{bmatrix}
m_1 \\
m_2 \\
m_3 \\
\end{bmatrix} =
\mathbf{B} \begin{bmatrix}
n_1 \\
n_2 \\
n_3 \\
\end{bmatrix}
$$

因此坐标变换公式为，

$$
\begin{bmatrix}
n_1 \\
n_2 \\
n_3 \\
\end{bmatrix} =
\mathbf{B}^{-1} \mathbf{A} \begin{bmatrix}
m_1 \\
m_2 \\
m_3 \\
\end{bmatrix} =
\mathbf{P}^{-1} \begin{bmatrix}
m_1 \\
m_2 \\
m_3 \\
\end{bmatrix}
$$

## 4.4 线性变换


